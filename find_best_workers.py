import time
import os
import multiprocessing
from concurrent.futures import ProcessPoolExecutor
import numpy as np
import onnxruntime as ort
import re

# VITS ëª¨ë“ˆ ì„í¬íŠ¸
import utils
import commons
from text import text_to_sequence
from text.k2j import korean2katakana
from text.j2k import japanese2korean

# ==========================================
# [ì„¤ì •]
# ==========================================
MODEL_PATH = "ko.onnx"       # ì–‘ìí™” ëª¨ë¸ì´ ìˆë‹¤ë©´ ko_quant.onnx ë¡œ ë³€ê²½ ê¶Œì¥
CONFIG_PATH = "./configs/ko.json"
TEST_TEXT = "[KO]RTF ìµœì í™”ë¥¼ ìœ„í•œ ì›Œì»¤ ìˆ˜ ë³„ ì„±ëŠ¥ ì¸¡ì • í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.[KO]"

# í…ŒìŠ¤íŠ¸í•  ì›Œì»¤ ìˆ˜ ëª©ë¡ (ì„œë²„ ì‚¬ì–‘ì— ë§ì¶° ìë™ ì„¤ì •ë¨)
# ì˜ˆ: [1, 2, 4, 8, 12, 16, ...]
cpu_count = os.cpu_count()
WORKER_CANDIDATES = sorted(list(set([1, 2, 4, 8, 12, 16, 24, 32, cpu_count, cpu_count-2])))
WORKER_CANDIDATES = [w for w in WORKER_CANDIDATES if w <= cpu_count and w > 0]

CALLS_PER_WORKER = 5  # ê° ì›Œì»¤ë‹¹ ì²˜ë¦¬í•  ìš”ì²­ ìˆ˜ (ì´ ìš”ì²­ = ì›Œì»¤ ìˆ˜ * ì´ ê°’)

# ì „ì—­ ë³€ìˆ˜
ort_session = None
hps = None

def get_text(text, hps):
    text_norm = text_to_sequence(text, hps.data.text_cleaners)
    if hps.data.add_blank:
        text_norm = commons.intersperse(text_norm, 0)
    text_norm = np.array(text_norm, dtype=np.int64)
    return text_norm

def init_worker():
    global ort_session, hps
    try:
        # ìŠ¤ë ˆë“œ ê²½í•© ë°©ì§€ (ë§¤ìš° ì¤‘ìš”)
        os.environ["OMP_NUM_THREADS"] = "1"
        os.environ["MKL_NUM_THREADS"] = "1"
        
        hps = utils.get_hparams_from_file(CONFIG_PATH)
        
        sess_options = ort.SessionOptions()
        sess_options.intra_op_num_threads = 1
        sess_options.inter_op_num_threads = 1
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        ort_session = ort.InferenceSession(MODEL_PATH, sess_options, providers=['CPUExecutionProvider'])
    except:
        pass

def run_inference(dummy_idx):
    global ort_session, hps
    try:
        # ì „ì²˜ë¦¬
        text_proc = re.sub('[\n]', '', TEST_TEXT).strip()
        if hasattr(hps.data, 'is_japanese_dataset') and hps.data.is_japanese_dataset:
            text_proc = re.sub(r'\[KO\](.*?)\[KO\]', lambda x: korean2katakana(x.group(1)), text_proc)
        elif hasattr(hps.data, 'is_korean_dataset') and hps.data.is_korean_dataset:
            text_proc = re.sub(r'\[JA\](.*?)\[JA\]', lambda x: japanese2korean(x.group(1)), text_proc)

        stn_tst = get_text(text_proc, hps)
        x_tst = np.expand_dims(stn_tst, axis=0)
        x_tst_lengths = np.array([x_tst.shape[1]], dtype=np.int64)

        inputs = {
            'text': x_tst,
            'text_lengths': x_tst_lengths,
            'noise_scale': np.array([0.667], dtype=np.float32),
            'length_scale': np.array([1.0], dtype=np.float32),
        }

        # ì¶”ë¡ 
        t1 = time.time()
        audio = ort_session.run(None, inputs)[0]
        t2 = time.time()
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´
        audio = audio.squeeze()
        duration = len(audio) / hps.data.sampling_rate
        
        return t2 - t1, duration # ì†Œìš”ì‹œê°„, ì˜¤ë””ì˜¤ê¸¸ì´
    except Exception as e:
        return None

def benchmark(num_workers):
    print(f"\n>>> í…ŒìŠ¤íŠ¸ ì¤‘: ì›Œì»¤ {num_workers}ê°œ ...", end="", flush=True)
    
    total_calls = num_workers * CALLS_PER_WORKER
    executor = ProcessPoolExecutor(max_workers=num_workers, initializer=init_worker)
    
    # ì›œì—…
    warmups = [executor.submit(run_inference, -1) for _ in range(num_workers)]
    for f in warmups: f.result()
    
    # ì‹¤ì œ ì¸¡ì •
    start_total = time.time()
    futures = [executor.submit(run_inference, i) for i in range(total_calls)]
    results = [f.result() for f in futures]
    end_total = time.time()
    
    executor.shutdown()
    
    valid_results = [r for r in results if r is not None]
    if not valid_results: return 0, 0, 0

    inference_times = [r[0] for r in valid_results]
    audio_durations = [r[1] for r in valid_results]
    
    avg_rtf = (sum(inference_times) / len(inference_times)) / (sum(audio_durations) / len(audio_durations))
    elapsed = end_total - start_total
    total_audio = sum(audio_durations)
    throughput = total_audio / elapsed # ì´ˆë‹¹ ìƒì„± ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì‹œê°„ (ë°°ì†)
    
    print(f" ì™„ë£Œ! (RTF: {avg_rtf:.4f} / ì²˜ë¦¬ëŸ‰: {throughput:.2f}ë°°ì†)")
    return avg_rtf, throughput

if __name__ == "__main__":
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except: pass
    
    print(f"=== ìµœì ì˜ ì›Œì»¤ ìˆ˜ ì°¾ê¸° (CPU: {os.cpu_count()} cores) ===")
    print(f"ëª¨ë¸: {MODEL_PATH}")
    
    best_throughput = 0
    best_worker_tp = 0
    
    results = []
    
    for w in WORKER_CANDIDATES:
        rtf, throughput = benchmark(w)
        results.append((w, rtf, throughput))
        
        if throughput > best_throughput:
            best_throughput = throughput
            best_worker_tp = w
            
    print("\n\n=== ğŸ“Š ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸ ===")
    print(f"{'ì›Œì»¤ ìˆ˜':<10} | {'í‰ê·  RTF (ì‘ë‹µì†ë„)':<20} | {'ì²˜ë¦¬ëŸ‰ (ë™ì‹œì²˜ë¦¬ë ¥)':<20}")
    print("-" * 60)
    for w, rtf, tp in results:
        mark = "â­ (Best)" if w == best_worker_tp else ""
        # RTFê°€ 0.15 ì´í•˜ì´ë©´ì„œ ì²˜ë¦¬ëŸ‰ì´ ë†’ì€ êµ¬ê°„ì´ ì‹¤ì‚¬ìš©ì— ê°€ì¥ ì¢‹ìŠµë‹ˆë‹¤.
        quality = "ì¾Œì " if rtf < 0.15 else ("ë³´í†µ" if rtf < 0.5 else "ëŠë¦¼")
        print(f"{w:<10} | {rtf:.4f} ({quality})      | {tp:.2f}x {mark}")
    
    print("-" * 60)
    print(f"âœ… ì¶”ì²œ ì„¤ì •: NUM_WORKERS = {best_worker_tp}")
    print("   (ì´ ì„¤ì •ì´ ì„œë²„ ìì›ì„ ìµœëŒ€ë¡œ í™œìš©í•˜ì—¬ ê°€ì¥ ë§ì€ ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤)")
    print("   ë§Œì•½ 'ê°œë³„ ì‘ë‹µ ì†ë„'ê°€ ë” ì¤‘ìš”í•˜ë‹¤ë©´ RTF < 0.1 ì¸ ì›Œì»¤ ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")